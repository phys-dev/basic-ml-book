# Линейная регрессия

## Постановка задачи

Линейная регрессия решает задачу предсказания непрерывной целевой переменной $y$ на основе одной или нескольких входных переменных (признаков) $x$.

В простейшем одномерном случае модель имеет вид:

$$\hat{y} = w_1 x + w_0$$

где:
- $w_1$ — вес признака (наклон прямой),
- $w_0$ — смещение (bias, свободный член).

Модель предполагает, что истинная зависимость имеет вид:

$$y = w_1 x + w_0 + \varepsilon$$

где $\varepsilon$ — случайная ошибка (шум).

**Цель обучения:** найти такие параметры $w_0, w_1, \dots, w_k$, чтобы предсказания модели $\hat{y}$ были максимально близки к реальным значениям $y$ из обучающей выборки.

## Функции ошибки

Для оценки качества модели используются следующие функции потерь:

- **SSE (Sum of Squared Errors)** — сумма квадратов ошибок:
  $$\text{SSE} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 = \|\text{error}\|_2^2$$

- **MSE (Mean Squared Error)** — среднеквадратичная ошибка:
  $$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

- **MAE (Mean Absolute Error)** — средняя абсолютная ошибка:
  $$\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$

MSE чаще используется в линейной регрессии, так как является дифференцируемой функцией и приводит к аналитическому решению.

## Матричная форма

Для многомерного случая ($k$ признаков) модель записывается как:

$$y_i = w_1 x_{1,i} + w_2 x_{2,i} + \dots + w_k x_{k,i} + w_0$$

Для удобства вычислений смещение $w_0$ включают в вектор весов, добавляя фиктивный признак $x_0 = 1$ ко всем объектам.

В матричной форме:

$$\mathbf{y} = \mathbf{X} \mathbf{w}$$

где:
- $\mathbf{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_N \end{bmatrix}$ — вектор целевых значений,
- $\mathbf{X} = \begin{bmatrix} x_{1,1} & \cdots & x_{1,k} & 1 \\ \vdots & \ddots & \vdots & \vdots \\ x_{N,1} & \cdots & x_{N,k} & 1 \end{bmatrix}$ — матрица объектов-признаков (с добавленным столбцом единиц),
- $\mathbf{w} = \begin{bmatrix} w_1 \\ \vdots \\ w_k \\ w_0 \end{bmatrix}$ — вектор весов.

**Оптимизационная задача:**

$$\hat{\mathbf{w}} = \arg\min_{\mathbf{w}} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2$$

## Аналитическое решение: метод наименьших квадратов (МНК)

Квадратичная функция потерь:

$$Q(\mathbf{w}) = \|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2 = (\mathbf{y} - \mathbf{X}\mathbf{w})^T (\mathbf{y} - \mathbf{X}\mathbf{w})$$

Для нахождения минимума приравниваем градиент к нулю:

$$\nabla_{\mathbf{w}} Q(\mathbf{w}) = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} = 0$$

Отсюда получаем **аналитическое решение**:

$$\hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

### Проблемы аналитического решения

1. **Матрица $\mathbf{X}^T \mathbf{X}$ может быть необратима:**
   - При линейной зависимости признаков (коллинеарность),
   - Когда число признаков $k$ превышает число объектов $N$ (бесконечное множество решений).

2. **Вычислительная сложность:** обращение матрицы имеет сложность $O(k^3)$, что непрактично при большом числе признаков.

## Градиентный спуск

Когда аналитическое решение неприменимо или неэффективно, используют итеративный метод оптимизации — градиентный спуск.

**Алгоритм:**

1. Инициализировать веса случайными значениями: $\mathbf{w} \gets \text{random}()$
2. Повторять до сходимости:
   $$\mathbf{w} \gets \mathbf{w} - \alpha \nabla_{\mathbf{w}} Q(\mathbf{w})$$

где:
- $\alpha$ — скорость обучения (learning rate),
- $\nabla_{\mathbf{w}} Q(\mathbf{w})$ — градиент функции потерь.

Для функции потерь MSE градиент вычисляется как:

$$\nabla_{\mathbf{w}} Q(\mathbf{w}) = \frac{2}{N} \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{y})$$

### Варианты градиентного спуска

В зависимости от количества объектов, используемых для вычисления градиента на каждой итерации:

- **Полный градиентный спуск (Batch GD):** градиент считается по всей выборке.
- **Стохастический градиентный спуск (SGD):** градиент считается по одному случайному объекту.
- **Мини-батч градиентный спуск:** градиент считается по небольшой случайной подвыборке (батчу).

### Проблемы и улучшения

- **Локальные минимумы:** для выпуклых функций (как MSE) проблема отсутствует — любой локальный минимум является глобальным.
- **Выбор скорости обучения $\alpha$:**
  - Слишком большая $\alpha$ — алгоритм расходится,
  - Слишком маленькая $\alpha$ — обучение слишком медленное.
  - Решение: адаптивное уменьшение $\alpha$ по мере обучения.
- **Momentum (инерция):**
  $$\mathbf{v} \gets \beta \mathbf{v} + \alpha \nabla_{\mathbf{w}} Q(\mathbf{w}), \quad \mathbf{w} \gets \mathbf{w} - \mathbf{v}$$
  где $\beta \in (0, 1)$ — коэффициент инерции. Ускоряет сходимость и помогает «проскакивать» пологие участки.

## Проблемы линейной регрессии и их решения

### 1. Отсутствие линейной зависимости

Если истинная зависимость нелинейна, линейная модель будет давать плохие предсказания.

**Пример:** предсказание стоимости склада по длине и ширине. Линейная модель:
$$\text{цена} = w_1 \cdot \text{длина} + w_2 \cdot \text{ширина} + w_0$$
не учитывает, что важна площадь ($\text{длина} \times \text{ширина}$). Решение — создать новый признак «площадь».

### 2. Коллинеарность признаков

Если признаки линейно зависимы ($x_2 = 2x_1$), решение становится неустойчивым и неинтерпретируемым:

$$y = 6x_1 + 8x_2 + 5 = 0x_1 + 11x_2 + 5 = 22x_1 + 0x_2 + 5 = \dots$$

**Решение:** удалять линейно зависимые признаки (анализ корреляции, методы отбора признаков).

### 3. Выбросы

Выбросы сильно влияют на MSE из-за квадратичного штрафа.

**Решения:**
- Фильтрация выбросов на этапе предобработки,
- Использование более устойчивых функций потерь (например, MAE или Huber loss).

### 4. Шумные признаки и переобучение

Если в модели присутствуют нерелевантные признаки, модель может выучить большие веса для них, что приведёт к переобучению.

**Решение:** регуляризация.

## Регуляризация

Регуляризация добавляет штраф за большие веса в функцию потерь:

### L2-регуляризация (Ridge)

$$Q_{\text{reg}}(\mathbf{w}) = \frac{1}{N} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2 + \lambda \|\mathbf{w}\|_2^2$$

- «Выравнивает» веса, делая их меньше по модулю,
- Даёт более стабильное решение при коллинеарности,
- Аналитическое решение: $\hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}$.

### L1-регуляризация (Lasso)

$$Q_{\text{reg}}(\mathbf{w}) = \frac{1}{N} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2 + \lambda \|\mathbf{w}\|_1$$

- Также уменьшает веса,
- **Обнуляет веса нерелевантных признаков**, выполняя автоматический отбор признаков.

### Параметр регуляризации $\lambda$

- $\lambda = 0$: обычная линейная регрессия,
- $\lambda \to \infty$: все веса стремятся к нулю,
- Оптимальное значение $\lambda$ подбирается на валидационной выборке.

## Параметры и гиперпараметры

| Тип | Примеры |
|-----|---------|
| **Параметры модели** | Веса $\mathbf{w}$ (настраиваются в процессе обучения) |
| **Гиперпараметры** | Скорость обучения $\alpha$, тип и коэффициент регуляризации $\lambda$, начальное приближение, использование momentum, размер батча |

## Оценка качества модели

1. **Метрики ошибки:** MSE, MAE на тестовой выборке.
2. **Базовое сравнение:** сравнение с «наивной» моделью $\hat{y} = \text{mean}(\mathbf{y})$.
3. **Разделение выборки:** разбиение на обучающую и тестовую части (train/test split).
4. **Кросс-валидация:** более надёжная оценка при ограниченном объёме данных.

---

> В следующих главах: логистическая регрессия (классификация), метрики качества для задач классификации, деревья решений.