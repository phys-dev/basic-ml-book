<!DOCTYPE HTML>
<html lang="ru" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Базовые методы искусственного интеллекта в физических исследованиях</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro/intro.html">О книге</a></li><li class="chapter-item expanded affix "><a href="intro/about-me.html">О себе</a></li><li class="chapter-item expanded affix "><li class="part-title">Немного теории</li><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Введение</a></li><li class="chapter-item expanded "><a href="metric-algo.html"><strong aria-hidden="true">2.</strong> Метрические алгоритмы</a></li><li class="chapter-item expanded "><a href="linear-regression.html"><strong aria-hidden="true">3.</strong> Линейная регрессия</a></li><li class="chapter-item expanded "><a href="logistic-regression-and-trees.html"><strong aria-hidden="true">4.</strong> Логистическая регрессия и деревья</a></li><li class="chapter-item expanded affix "><li class="part-title">Работаем самостоятельно</li><li class="chapter-item expanded "><a href="practicum/knn-task.html"><strong aria-hidden="true">5.</strong> Пишем свой kNN</a></li><li class="chapter-item expanded "><a href="practicum/linear-regression-task.html"><strong aria-hidden="true">6.</strong> Пишем свою линейную регрессию</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Базовые методы искусственного интеллекта в физических исследованиях</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/phys-dev/basic-ml-book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="Базовые-методы-искусственного-интеллекта-в-физических-исследованиях"><a class="header" href="#Базовые-методы-искусственного-интеллекта-в-физических-исследованиях">Базовые методы искусственного интеллекта в физических исследованиях</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="О-себе"><a class="header" href="#О-себе">О себе</a></h1>
<p>Меня зовут Федоров Вячеслав Васильевич, я разработчик программного обеспечения с глубокими знаниями в области физики и вычислительной математики. На протяжении нескольких лет я работаю в Институте ядерной физики им. Будкера, где занимаюсь разработкой наукоемкого прикладного программного обеспечения на языках высокого уровня Python и C++ для решения различных задач. Моя основная работа сосредоточена на моделировании динамики заряженных частиц в сложных электромагнитных полях, а также на внедрении алгоритмов машинного обучения для оптимизации и настройки ускорительных комплексов.</p>
<p>Мой опыт также включает работу в международной компании по разработке ПО и веб-приложений SIBERS, где я руководил командой разработчиков в создании ПО на основе микросервисной архитектуры для государственной организации, оказывающей финансовые услуги. Я активно участвовал в разработке дополнительных модулей для статического анализа кода для различных сред разработки, а также был ведущим разработчиком приложения для врачей, предназначенного для распознавания и присвоения кодов болезней в медицинских картах пациентов с использованием бессерверных вычислений и алгоритмов машинного обучения. Я внедрял процессы автоматизированного тестирования, непрерывной интеграции и доставки кода, проводил обзор и оценку кода, а также подготовил и прочитал полугодовой обучающий курс «Микросервисные масштабируемые веб-сайты» для команды.</p>
<p>Ранее я принимал участие в проектах Роскосмоса, в том числе в разработке ПО на языке C++ для инфракрасного датчика горизонта с использованием платформы Arduino для сверхмалого космического аппарата НГУ «Норби», успешный запуск которого состоялся в 2020 году.</p>
<p>У меня есть диплом бакалавра НГУ в области физики пучков заряженных частиц и физики ускорителей. Я прошёл курсы повышения квалификации по разработке и эксплуатации ПО на Python и C++, алгоритмам и структурам данных, системному администрированию и обеспечению надёжности информационных систем от «Образовательных технологий Яндекса», а также основы искусственного интеллекта и машинного обучения от НГУ.</p>
<p>Мои технические навыки включают: отличное владение языками программирования Python и C++, разнообразными фреймворками; работу с асинхронным и параллельным кодом; проектирование ПО, веб-приложений и микросервисов; создание документации; работу с реляционными базами данных и NoSQL-хранилищами; управление очередями сообщений и задач; владение методологиями непрерывной интеграции и доставки кода, автоматизации процессов сборки, настройки и развёртывания ПО. Мой опыт позволяет ускорять процессы производства IT-продуктов за счёт поиска и устранения «узких» мест, автоматизировать процесс разработки и развёртывания приложений, контейнеризировать приложения и размещать их в облачных сервисах. Я использую актуальные инструменты для обеспечения качества, скорости и стабильности приложений, управляю инфраструктурой в парадигме Infrastructure as Code, сокращая время команды на развёртывание и масштабирование, а также налаживаю коммуникацию между участниками процесса разработки продукта: службой эксплуатации, разработчиками, заказчиками от бизнеса и многими другими.</p>
<blockquote>
<p>Больше обо мне можно узнать <a href="https://fuodorov.github.io">здесь</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="Введение"><a class="header" href="#Введение">Введение</a></h1>
<h2 id="Цели-курса"><a class="header" href="#Цели-курса">Цели курса</a></h2>
<p>Этот курс станет вашим стартом в мир <strong>Data Science</strong> и <strong>Machine Learning</strong>. По его окончании вы сможете претендовать на позиции <strong>Junior Machine Learning Engineer</strong> или <strong>Junior Data Scientist</strong>.</p>
<h3 id="Для-вас-этот-курс--это"><a class="header" href="#Для-вас-этот-курс--это">Для вас этот курс — это:</a></h3>
<ul>
<li><strong>Практическое применение</strong> знаний из математического анализа, статистики и линейной алгебры.</li>
<li>Фундамент для будущего перехода на <strong>руководящие должности</strong> в технической сфере.</li>
</ul>
<h3 id="Для-нас-этот-курс--это"><a class="header" href="#Для-нас-этот-курс--это">Для нас этот курс — это:</a></h3>
<ul>
<li>Развитие профессионального сообщества.</li>
<li>Возможность найти будущих коллег.</li>
<li>Интересный и важный процесс поддержания знаний в актуальном состоянии.</li>
</ul>
<h2 id="Основные-термины"><a class="header" href="#Основные-термины">Основные термины</a></h2>
<p>Давайте договоримся о терминах, которые будут использоваться на протяжении всего курса:</p>
<ul>
<li><strong>Искусственный интеллект (AI)</strong> — система, способная принимать решения на основе восприятия окружающего мира.</li>
<li><strong>Машинное обучение (ML)</strong> — подраздел AI; система, принимающая решения на основе <strong>накопленного опыта (данных)</strong> и текущего состояния мира.</li>
<li><strong>Глубокое обучение (DL)</strong> — подраздел ML, основанный на использовании глубоких <strong>нейронных сетей (Neural Networks, NN)</strong>.</li>
<li><strong>Data Science</strong> — дисциплина, включающая сбор, обработку, анализ и извлечение знаний из данных.</li>
<li><strong>Big Data</strong> — обработка и анализ данных, масштаб которых не позволяет работать с ними в стандартных инструментах (например, в Excel).</li>
</ul>
<h3 id="Термины-связанные-с-данными"><a class="header" href="#Термины-связанные-с-данными">Термины, связанные с данными</a></h3>
<ul>
<li><strong>Датасет / Выборка (Dataset)</strong> — набор данных, с которым работает алгоритм.</li>
<li><strong>Признак / Фича (Feature, X)</strong> — характеристика или измеряемый параметр объекта.</li>
<li><strong>Целевая переменная / Метка / Класс (Label, Target, y)</strong> — значение, которое мы хотим предсказать по признакам объекта.</li>
<li><strong>Закон природы (в контексте ML)</strong> — скрытая взаимосвязь между признаками и целевой переменной, которую стремится восстановить модель. Формально: отображение из пространства признаков <code>X</code> в пространство меток <code>y</code>.</li>
</ul>
<h2 id="Типы-задач-машинного-обучения"><a class="header" href="#Типы-задач-машинного-обучения">Типы задач машинного обучения</a></h2>
<p>Задачи ML классифицируются по наличию и типу разметки (метки <code>y</code>).</p>
<h3 id="1-Обучение-с-учителем-supervised-learning"><a class="header" href="#1-Обучение-с-учителем-supervised-learning">1. Обучение с учителем (Supervised Learning)</a></h3>
<p>Есть <strong>размеченная</strong> обучающая выборка, где каждому объекту сопоставлена правильная метка <code>y</code>. Цель — восстановить закон природы <code>X -&gt; y</code>.</p>
<p><strong>Пример:</strong> Классификация электронных писем (спам / не спам), где человек вручную разметил исторические данные.</p>
<h3 id="2-Обучение-без-учителя-unsupervised-learning"><a class="header" href="#2-Обучение-без-учителя-unsupervised-learning">2. Обучение без учителя (Unsupervised Learning)</a></h3>
<p>Разметки <code>y</code> нет или она не используется. Алгоритм ищет внутренние структуры, закономерности и связи в данных.</p>
<p><strong>Пример:</strong> Кластеризация пользователей поисковой системы по их запросам для выявления групп интересов.</p>
<h2 id="Основные-типы-задач"><a class="header" href="#Основные-типы-задач">Основные типы задач</a></h2>
<p>В рамках двух парадигм выделяют несколько ключевых типов задач.</p>
<h3 id="Классификация-classification"><a class="header" href="#Классификация-classification">Классификация (Classification)</a></h3>
<ul>
<li><strong>Цель:</strong> Восстановить закон природы.</li>
<li><strong>Особенность:</strong> Множество меток <code>y</code> — <strong>конечное</strong> (часто небольшое).</li>
<li><strong>Подвиды:</strong> Бинарная (2 класса) и многоклассовая (&gt;2 классов).</li>
<li><strong>Пример:</strong> Определение болезни по симптомам (болен/здоров), распознавание цифр на изображении.</li>
</ul>
<h3 id="Регрессия-regression"><a class="header" href="#Регрессия-regression">Регрессия (Regression)</a></h3>
<ul>
<li><strong>Цель:</strong> Предсказать <strong>непрерывную числовую</strong> величину.</li>
<li><strong>Особенность:</strong> Метка <code>y</code> — вещественное число.</li>
<li><strong>Пример:</strong> Прогнозирование стоимости квартиры по её характеристикам, прогноз температуры на завтра.</li>
</ul>
<h3 id="Кластеризация-clustering"><a class="header" href="#Кластеризация-clustering">Кластеризация (Clustering)</a></h3>
<ul>
<li><strong>Цель:</strong> Разбить данные на группы (кластеры) так, чтобы объекты внутри одной группы были <strong>похожи</strong>, а объекты из разных групп — <strong>отличались</strong>.</li>
<li><strong>Особенность:</strong> Отсутствие заранее известных меток (без учителя).</li>
<li><strong>Пример:</strong> Сегментация клиентов для маркетинга, группировка новостей по темам.</li>
</ul>
<h3 id="Снижение-размерности-dimensionality-reduction"><a class="header" href="#Снижение-размерности-dimensionality-reduction">Снижение размерности (Dimensionality Reduction)</a></h3>
<ul>
<li><strong>Цель:</strong> Уменьшить количество признаков, перейдя в пространство меньшей размерности, сохранив при этом важные структуры данных (близкие объекты должны остаться близкими).</li>
<li><strong>Применение:</strong> Визуализация данных (например, 3D -&gt; 2D), борьба с <strong>"проклятием размерности"</strong>, сжатие данных.</li>
</ul>
<h3 id="Ранжирование-ranking"><a class="header" href="#Ранжирование-ranking">Ранжирование (Ranking)</a></h3>
<ul>
<li><strong>Цель:</strong> Упорядочить объекты (например, документы или товары) согласно их релевантности запросу или предпочтениям пользователя.</li>
<li><strong>Пример:</strong> Выдача результатов поиска, рекомендательные системы.</li>
</ul>
<h3 id="Генерация-generation"><a class="header" href="#Генерация-generation">Генерация (Generation)</a></h3>
<ul>
<li><strong>Цель:</strong> Создавать новые объекты (изображения, текст, музыку), похожие на объекты из обучающей выборки.</li>
<li><strong>Пример:</strong> Генерация реалистичных лиц, написание текстов в стиле определённого автора.</li>
</ul>
<h2 id="Типы-признаков-features"><a class="header" href="#Типы-признаков-features">Типы признаков (Features)</a></h2>
<p>Признаки могут быть разных типов, что влияет на выбор модели и методов предобработки:</p>
<ul>
<li><strong>Бинарные:</strong> Выбор из двух вариантов (Да/Нет, Кот/Не кот).</li>
<li><strong>Номинальные (категориальные):</strong> Конечное множество без порядка (цвета, марки машин).</li>
<li><strong>Порядковые (ординальные):</strong> Конечное упорядоченное множество (оценки: плохо/удовлетворительно/хорошо/отлично).</li>
<li><strong>Числовые (вещественные):</strong> Непрерывные величины (рост, цена, расстояние).</li>
</ul>
<p><strong>Важно:</strong> Типы признаков можно преобразовывать (например, разбить числовой на интервалы и получить порядковый).</p>
<h2 id="Что-такое-модель"><a class="header" href="#Что-такое-модель">Что такое модель?</a></h2>
<p><strong>Модель машинного обучения</strong> — это параметрическая функция (или "чёрный ящик"), которая отображает пространство признаков <code>X</code> в пространство ответов <code>y</code>: <code>Model: X -&gt; y</code>.</p>
<ul>
<li>У модели есть <strong>параметры</strong> (внутренние настройки), которые настраиваются в процессе <strong>обучения</strong> на данных.</li>
<li>Обучение — это процесс подбора параметров модели таким образом, чтобы её предсказания на обучающих данных максимально соответствовали известным меткам (или выявляли скрытые структуры).</li>
<li>На работу модели влияют: качество данных, выбор алгоритма, его гиперпараметры и правильность процесса обучения.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="Метод-k-ближайших-соседей-knn"><a class="header" href="#Метод-k-ближайших-соседей-knn">Метод k-ближайших соседей (KNN)</a></h1>
<h2 id="Постановка-задачи"><a class="header" href="#Постановка-задачи">Постановка задачи</a></h2>
<p>Рассмотрим задачу классификации животных по двум признакам:</p>
<ul>
<li>Длина усов</li>
<li>Длина хвоста</li>
</ul>
<p>Для каждого объекта в обучающей выборке известна метка: <em>кот</em> или <em>пёс</em>. Цель — построить модель, которая по новым измерениям определит класс животного.</p>
<p>При визуализации данных наблюдается, что объекты одного класса группируются в определённых областях пространства признаков.</p>
<h2 id="Гипотеза-о-компактности"><a class="header" href="#Гипотеза-о-компактности">Гипотеза о компактности</a></h2>
<p>Основа метода KNN — <strong>гипотеза о компактности</strong>:</p>
<blockquote>
<p>Объекты одного класса расположены «близко» друг к другу в пространстве признаков, а объекты разных классов — «далеко».</p>
</blockquote>
<p>Эта гипотеза позволяет решать задачу классификации через поиск похожих (близких) объектов в обучающей выборке.</p>
<h2 id="Алгоритм-knn"><a class="header" href="#Алгоритм-knn">Алгоритм KNN</a></h2>
<p><strong>Определение:</strong><br />
K-ближайших соседей (K Nearest Neighbors, KNN) — один из самых простых и интуитивно понятных алгоритмов классификации.</p>
<p><strong>Алгоритм предсказания:</strong></p>
<ol>
<li>Для нового объекта вычислить расстояние до всех объектов обучающей выборки</li>
<li>Выбрать $K$ объектов с наименьшим расстоянием</li>
<li>Присвоить объекту класс, который чаще всего встречается среди $K$ соседей (голосование большинства)</li>
</ol>
<p><strong>Гиперпараметр:</strong><br />
$K$ — количество соседей, участвующих в голосовании. Выбор $K$ влияет на качество модели:</p>
<ul>
<li>Малое $K$: модель чувствительна к шуму и выбросам</li>
<li>Большое $K$: граница решений становится более гладкой, но может потерять локальные особенности</li>
</ul>
<h2 id="Метрики-расстояния"><a class="header" href="#Метрики-расстояния">Метрики расстояния</a></h2>
<p>Для определения «близости» объектов используются различные метрики:</p>
<h3 id="Манхэттенское-расстояние"><a class="header" href="#Манхэттенское-расстояние">Манхэттенское расстояние</a></h3>
<p>$$d(\mathbf{x}, \mathbf{\hat{x}}) = \sum_{i=1}^{N} |x_i - \hat{x}_i|$$</p>
<h3 id="Евклидово-расстояние"><a class="header" href="#Евклидово-расстояние">Евклидово расстояние</a></h3>
<p>$$d(\mathbf{x}, \mathbf{\hat{x}}) = \sqrt{\sum_{i=1}^{N} (x_i - \hat{x}_i)^2}$$</p>
<h3 id="Косинусное-расстояние"><a class="header" href="#Косинусное-расстояние">Косинусное расстояние</a></h3>
<p>$$d(\mathbf{x}, \mathbf{\hat{x}}) = 1 - \frac{\sum_{i=1}^{N} x_i \hat{x}<em>i}{\sqrt{\sum</em>{i=1}^{N} x_i^2} \cdot \sqrt{\sum_{i=1}^{N} \hat{x}_i^2}}$$</p>
<p><strong>Преимущество косинусного расстояния:</strong> измеряет угол между векторами, а не абсолютную разницу значений. Полезно, когда важна ориентация вектора признаков, а не его длина.</p>
<h2 id="Проблемы-и-решения"><a class="header" href="#Проблемы-и-решения">Проблемы и решения</a></h2>
<h3 id="1-Зависимость-от-масштаба-признаков"><a class="header" href="#1-Зависимость-от-масштаба-признаков">1. Зависимость от масштаба признаков</a></h3>
<p><strong>Проблема:</strong><br />
Если признаки имеют разные масштабы (например, 29 признаков ∈ [0, 1], а один ∈ [0, 1000]), то расстояние будет доминироваться признаком с большим масштабом.</p>
<p><strong>Решение — нормализация признаков:</strong></p>
<ul>
<li>Минимакс-нормализация: приведение всех значений к диапазону [0, 1]</li>
<li>Стандартизация: приведение к нулевому математическому ожиданию и единичной дисперсии ($\mu = 0, \sigma = 1$)</li>
</ul>
<h3 id="2-Вычислительная-сложность"><a class="header" href="#2-Вычислительная-сложность">2. Вычислительная сложность</a></h3>
<p><strong>Проблема:</strong><br />
При большом объёме обучающей выборки ($N$ объектов) поиск ближайших соседей требует $O(N)$ операций сравнения для каждого нового объекта.</p>
<p><strong>Решение — структуры данных для ускорения поиска:</strong></p>
<ul>
<li><strong>kD-tree</strong> (k-dimensional tree): дерево разбиения пространства, на каждом уровне разделяющее данные по одному признаку</li>
<li><strong>Ball tree</strong>: иерархическая структура на основе гиперсфер</li>
<li><strong>HNSW</strong> (Hierarchical Navigable Small World): графовая структура для приближённого поиска ближайших соседей</li>
<li><strong>FRiS-Stolp</strong>: метод отбора эталонных объектов для сокращения размера выборки</li>
</ul>
<h3 id="3-Улучшение-голосования"><a class="header" href="#3-Улучшение-голосования">3. Улучшение голосования</a></h3>
<p>Вместо простого подсчёта количества соседей каждого класса можно использовать <strong>взвешенное голосование</strong>, где вес соседа обратно пропорционален расстоянию до него:</p>
<p>$$\text{вес}_i = \frac{1}{d(\mathbf{x}, \mathbf{x}_i)} \quad \text{или} \quad \text{вес}_i = e^{-d(\mathbf{x}, \mathbf{x}_i)}$$</p>
<h2 id="Свойства-модели-knn"><a class="header" href="#Свойства-модели-knn">Свойства модели KNN</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Аспект</th><th>Описание</th></tr></thead><tbody>
<tr><td><strong>Обучение</strong></td><td>Отсутствует в классическом смысле. Модель «запоминает» всю обучающую выборку</td></tr>
<tr><td><strong>Предсказание</strong></td><td>Вычислительно затратно: требуется рассчитать расстояния до всех объектов выборки</td></tr>
<tr><td><strong>Параметры</strong></td><td>Отсутствуют (модель не имеет обучаемых параметров)</td></tr>
<tr><td><strong>Гиперпараметры</strong></td><td>$K$ (количество соседей), тип метрики расстояния, стратегия взвешивания</td></tr>
<tr><td><strong>Интерпретируемость</strong></td><td>Высокая: решение принимается на основе конкретных похожих объектов</td></tr>
</tbody></table>
</div>
<h2 id="Метод-fris-stolp-для-отбора-эталонов"><a class="header" href="#Метод-fris-stolp-для-отбора-эталонов">Метод FRiS-Stolp для отбора эталонов</a></h2>
<p>Для сокращения вычислительной сложности можно отобрать подмножество наиболее информативных объектов — <strong>эталонов</strong> (столпов).</p>
<p><strong>Критерий качества эталона:</strong><br />
Объект является хорошим эталоном своего класса, если:</p>
<ul>
<li>Объекты его класса расположены максимально близко к нему</li>
<li>Объекты других классов расположены максимально далеко</li>
</ul>
<p><strong>Функция FRiS:</strong>
$$\text{FRiS}(z, a_i, b_i) = \frac{r_2 - r_1}{r_2 + r_1}$$</p>
<p>где $r_1$ — расстояние до ближайшего объекта своего класса, $r_2$ — до ближайшего объекта чужого класса.</p>
<hr />
<blockquote>
<p>В следующих главах: метрики качества моделей машинного обучения, линейная регрессия.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="Линейная-регрессия"><a class="header" href="#Линейная-регрессия">Линейная регрессия</a></h1>
<h2 id="Постановка-задачи-1"><a class="header" href="#Постановка-задачи-1">Постановка задачи</a></h2>
<p>Линейная регрессия решает задачу предсказания непрерывной целевой переменной $y$ на основе одной или нескольких входных переменных (признаков) $x$.</p>
<p>В простейшем одномерном случае модель имеет вид:</p>
<p>$$\hat{y} = w_1 x + w_0$$</p>
<p>где:</p>
<ul>
<li>$w_1$ — вес признака (наклон прямой),</li>
<li>$w_0$ — смещение (bias, свободный член).</li>
</ul>
<p>Модель предполагает, что истинная зависимость имеет вид:</p>
<p>$$y = w_1 x + w_0 + \varepsilon$$</p>
<p>где $\varepsilon$ — случайная ошибка (шум).</p>
<p><strong>Цель обучения:</strong> найти такие параметры $w_0, w_1, \dots, w_k$, чтобы предсказания модели $\hat{y}$ были максимально близки к реальным значениям $y$ из обучающей выборки.</p>
<h2 id="Функции-ошибки"><a class="header" href="#Функции-ошибки">Функции ошибки</a></h2>
<p>Для оценки качества модели используются следующие функции потерь:</p>
<ul>
<li>
<p><strong>SSE (Sum of Squared Errors)</strong> — сумма квадратов ошибок:
$$\text{SSE} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 = |\text{error}|_2^2$$</p>
</li>
<li>
<p><strong>MSE (Mean Squared Error)</strong> — среднеквадратичная ошибка:
$$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$</p>
</li>
<li>
<p><strong>MAE (Mean Absolute Error)</strong> — средняя абсолютная ошибка:
$$\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$</p>
</li>
</ul>
<p>MSE чаще используется в линейной регрессии, так как является дифференцируемой функцией и приводит к аналитическому решению.</p>
<h2 id="Матричная-форма"><a class="header" href="#Матричная-форма">Матричная форма</a></h2>
<p>Для многомерного случая ($k$ признаков) модель записывается как:</p>
<p>$$y_i = w_1 x_{1,i} + w_2 x_{2,i} + \dots + w_k x_{k,i} + w_0$$</p>
<p>Для удобства вычислений смещение $w_0$ включают в вектор весов, добавляя фиктивный признак $x_0 = 1$ ко всем объектам.</p>
<p>В матричной форме:</p>
<p>$$\mathbf{y} = \mathbf{X} \mathbf{w}$$</p>
<p>где:</p>
<ul>
<li>$\mathbf{y} = \begin{bmatrix} y_1 \ \vdots \ y_N \end{bmatrix}$ — вектор целевых значений,</li>
<li>$\mathbf{X} = \begin{bmatrix} x_{1,1} &amp; \cdots &amp; x_{1,k} &amp; 1 \ \vdots &amp; \ddots &amp; \vdots &amp; \vdots \ x_{N,1} &amp; \cdots &amp; x_{N,k} &amp; 1 \end{bmatrix}$ — матрица объектов-признаков (с добавленным столбцом единиц),</li>
<li>$\mathbf{w} = \begin{bmatrix} w_1 \ \vdots \ w_k \ w_0 \end{bmatrix}$ — вектор весов.</li>
</ul>
<p><strong>Оптимизационная задача:</strong></p>
<p>$$\hat{\mathbf{w}} = \arg\min_{\mathbf{w}} |\mathbf{X}\mathbf{w} - \mathbf{y}|_2^2$$</p>
<h2 id="Аналитическое-решение-метод-наименьших-квадратов-МНК"><a class="header" href="#Аналитическое-решение-метод-наименьших-квадратов-МНК">Аналитическое решение: метод наименьших квадратов (МНК)</a></h2>
<p>Квадратичная функция потерь:</p>
<p>$$Q(\mathbf{w}) = |\mathbf{y} - \mathbf{X}\mathbf{w}|_2^2 = (\mathbf{y} - \mathbf{X}\mathbf{w})^T (\mathbf{y} - \mathbf{X}\mathbf{w})$$</p>
<p>Для нахождения минимума приравниваем градиент к нулю:</p>
<p>$$\nabla_{\mathbf{w}} Q(\mathbf{w}) = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} = 0$$</p>
<p>Отсюда получаем <strong>аналитическое решение</strong>:</p>
<p>$$\hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$</p>
<h3 id="Проблемы-аналитического-решения"><a class="header" href="#Проблемы-аналитического-решения">Проблемы аналитического решения</a></h3>
<ol>
<li>
<p><strong>Матрица $\mathbf{X}^T \mathbf{X}$ может быть необратима:</strong></p>
<ul>
<li>При линейной зависимости признаков (коллинеарность),</li>
<li>Когда число признаков $k$ превышает число объектов $N$ (бесконечное множество решений).</li>
</ul>
</li>
<li>
<p><strong>Вычислительная сложность:</strong> обращение матрицы имеет сложность $O(k^3)$, что непрактично при большом числе признаков.</p>
</li>
</ol>
<h2 id="Градиентный-спуск"><a class="header" href="#Градиентный-спуск">Градиентный спуск</a></h2>
<p>Когда аналитическое решение неприменимо или неэффективно, используют итеративный метод оптимизации — градиентный спуск.</p>
<p><strong>Алгоритм:</strong></p>
<ol>
<li>Инициализировать веса случайными значениями: $\mathbf{w} \gets \text{random}()$</li>
<li>Повторять до сходимости:
$$\mathbf{w} \gets \mathbf{w} - \alpha \nabla_{\mathbf{w}} Q(\mathbf{w})$$</li>
</ol>
<p>где:</p>
<ul>
<li>$\alpha$ — скорость обучения (learning rate),</li>
<li>$\nabla_{\mathbf{w}} Q(\mathbf{w})$ — градиент функции потерь.</li>
</ul>
<p>Для функции потерь MSE градиент вычисляется как:</p>
<p>$$\nabla_{\mathbf{w}} Q(\mathbf{w}) = \frac{2}{N} \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{y})$$</p>
<h3 id="Варианты-градиентного-спуска"><a class="header" href="#Варианты-градиентного-спуска">Варианты градиентного спуска</a></h3>
<p>В зависимости от количества объектов, используемых для вычисления градиента на каждой итерации:</p>
<ul>
<li><strong>Полный градиентный спуск (Batch GD):</strong> градиент считается по всей выборке.</li>
<li><strong>Стохастический градиентный спуск (SGD):</strong> градиент считается по одному случайному объекту.</li>
<li><strong>Мини-батч градиентный спуск:</strong> градиент считается по небольшой случайной подвыборке (батчу).</li>
</ul>
<h3 id="Проблемы-и-улучшения"><a class="header" href="#Проблемы-и-улучшения">Проблемы и улучшения</a></h3>
<ul>
<li><strong>Локальные минимумы:</strong> для выпуклых функций (как MSE) проблема отсутствует — любой локальный минимум является глобальным.</li>
<li><strong>Выбор скорости обучения $\alpha$:</strong>
<ul>
<li>Слишком большая $\alpha$ — алгоритм расходится,</li>
<li>Слишком маленькая $\alpha$ — обучение слишком медленное.</li>
<li>Решение: адаптивное уменьшение $\alpha$ по мере обучения.</li>
</ul>
</li>
<li><strong>Momentum (инерция):</strong>
$$\mathbf{v} \gets \beta \mathbf{v} + \alpha \nabla_{\mathbf{w}} Q(\mathbf{w}), \quad \mathbf{w} \gets \mathbf{w} - \mathbf{v}$$
где $\beta \in (0, 1)$ — коэффициент инерции. Ускоряет сходимость и помогает «проскакивать» пологие участки.</li>
</ul>
<h2 id="Проблемы-линейной-регрессии-и-их-решения"><a class="header" href="#Проблемы-линейной-регрессии-и-их-решения">Проблемы линейной регрессии и их решения</a></h2>
<h3 id="1-Отсутствие-линейной-зависимости"><a class="header" href="#1-Отсутствие-линейной-зависимости">1. Отсутствие линейной зависимости</a></h3>
<p>Если истинная зависимость нелинейна, линейная модель будет давать плохие предсказания.</p>
<p><strong>Пример:</strong> предсказание стоимости склада по длине и ширине. Линейная модель:
$$\text{цена} = w_1 \cdot \text{длина} + w_2 \cdot \text{ширина} + w_0$$
не учитывает, что важна площадь ($\text{длина} \times \text{ширина}$). Решение — создать новый признак «площадь».</p>
<h3 id="2-Коллинеарность-признаков"><a class="header" href="#2-Коллинеарность-признаков">2. Коллинеарность признаков</a></h3>
<p>Если признаки линейно зависимы ($x_2 = 2x_1$), решение становится неустойчивым и неинтерпретируемым:</p>
<p>$$y = 6x_1 + 8x_2 + 5 = 0x_1 + 11x_2 + 5 = 22x_1 + 0x_2 + 5 = \dots$$</p>
<p><strong>Решение:</strong> удалять линейно зависимые признаки (анализ корреляции, методы отбора признаков).</p>
<h3 id="3-Выбросы"><a class="header" href="#3-Выбросы">3. Выбросы</a></h3>
<p>Выбросы сильно влияют на MSE из-за квадратичного штрафа.</p>
<p><strong>Решения:</strong></p>
<ul>
<li>Фильтрация выбросов на этапе предобработки,</li>
<li>Использование более устойчивых функций потерь (например, MAE или Huber loss).</li>
</ul>
<h3 id="4-Шумные-признаки-и-переобучение"><a class="header" href="#4-Шумные-признаки-и-переобучение">4. Шумные признаки и переобучение</a></h3>
<p>Если в модели присутствуют нерелевантные признаки, модель может выучить большие веса для них, что приведёт к переобучению.</p>
<p><strong>Решение:</strong> регуляризация.</p>
<h2 id="Регуляризация"><a class="header" href="#Регуляризация">Регуляризация</a></h2>
<p>Регуляризация добавляет штраф за большие веса в функцию потерь:</p>
<h3 id="l2-регуляризация-ridge"><a class="header" href="#l2-регуляризация-ridge">L2-регуляризация (Ridge)</a></h3>
<p>$$Q_{\text{reg}}(\mathbf{w}) = \frac{1}{N} |\mathbf{X}\mathbf{w} - \mathbf{y}|_2^2 + \lambda |\mathbf{w}|_2^2$$</p>
<ul>
<li>«Выравнивает» веса, делая их меньше по модулю,</li>
<li>Даёт более стабильное решение при коллинеарности,</li>
<li>Аналитическое решение: $\hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}$.</li>
</ul>
<h3 id="l1-регуляризация-lasso"><a class="header" href="#l1-регуляризация-lasso">L1-регуляризация (Lasso)</a></h3>
<p>$$Q_{\text{reg}}(\mathbf{w}) = \frac{1}{N} |\mathbf{X}\mathbf{w} - \mathbf{y}|_2^2 + \lambda |\mathbf{w}|_1$$</p>
<ul>
<li>Также уменьшает веса,</li>
<li><strong>Обнуляет веса нерелевантных признаков</strong>, выполняя автоматический отбор признаков.</li>
</ul>
<h3 id="Параметр-регуляризации-lambda"><a class="header" href="#Параметр-регуляризации-lambda">Параметр регуляризации $\lambda$</a></h3>
<ul>
<li>$\lambda = 0$: обычная линейная регрессия,</li>
<li>$\lambda \to \infty$: все веса стремятся к нулю,</li>
<li>Оптимальное значение $\lambda$ подбирается на валидационной выборке.</li>
</ul>
<h2 id="Параметры-и-гиперпараметры"><a class="header" href="#Параметры-и-гиперпараметры">Параметры и гиперпараметры</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Тип</th><th>Примеры</th></tr></thead><tbody>
<tr><td><strong>Параметры модели</strong></td><td>Веса $\mathbf{w}$ (настраиваются в процессе обучения)</td></tr>
<tr><td><strong>Гиперпараметры</strong></td><td>Скорость обучения $\alpha$, тип и коэффициент регуляризации $\lambda$, начальное приближение, использование momentum, размер батча</td></tr>
</tbody></table>
</div>
<h2 id="Оценка-качества-модели"><a class="header" href="#Оценка-качества-модели">Оценка качества модели</a></h2>
<ol>
<li><strong>Метрики ошибки:</strong> MSE, MAE на тестовой выборке.</li>
<li><strong>Базовое сравнение:</strong> сравнение с «наивной» моделью $\hat{y} = \text{mean}(\mathbf{y})$.</li>
<li><strong>Разделение выборки:</strong> разбиение на обучающую и тестовую части (train/test split).</li>
<li><strong>Кросс-валидация:</strong> более надёжная оценка при ограниченном объёме данных.</li>
</ol>
<hr />
<blockquote>
<p>В следующих главах: логистическая регрессия (классификация), метрики качества для задач классификации, деревья решений.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="Логистическая-регрессия-и-Деревья-решений"><a class="header" href="#Логистическая-регрессия-и-Деревья-решений">Логистическая регрессия и Деревья решений</a></h1>
<p>В этой главе мы рассмотрим одни из фундаментальных методов машинного обучения для задач классификации: логистическую регрессию и деревья решений.
Также мы подробно разберем метрики качества, необходимые для оценки работы классификаторов, так как стандартная точность (accuracy) не всегда отражает реальную эффективность модели.</p>
<h2 id="31-Линейные-классификаторы-и-Логистическая-регрессия"><a class="header" href="#31-Линейные-классификаторы-и-Логистическая-регрессия">3.1. Линейные классификаторы и Логистическая регрессия</a></h2>
<h3 id="От-линейной-функции-к-вероятности"><a class="header" href="#От-линейной-функции-к-вероятности">От линейной функции к вероятности</a></h3>
<p>Линейный классификатор строит решающую границу в виде гиперплоскости. Для объекта с признаками $x$ модель вычисляет линейную комбинацию:</p>
<p>$$
f(x) = w_1 x_1 + w_2 x_2 + ... + w_0 = w^T x
$$</p>
<p>В простейшем случае класс предсказывается через знак функции:
$$
\hat{y}_i = \text{sign}(f(x_i)) =
\begin{cases}
1, &amp; f(x_i) \geq 0 \
-1, &amp; f(x_i) &lt; 0
\end{cases}
$$</p>
<p>Однако для многих задач важно получить не просто класс, а <strong>вероятность</strong> принадлежности к классу.
Возникает проблема: значение $f(x)$ лежит в диапазоне $(-\infty, +\infty)$,
а вероятность $p$ должна быть в диапазоне $[0, 1]$.</p>
<p>Чтобы преобразовать линейный отклик в вероятность, используется следующая цепочка рассуждений:</p>
<ol>
<li>Рассмотрим вероятность положительного класса $p_+ = P(y=1|x)$.</li>
<li>Преобразуем вероятность в <strong>шанс</strong> (Odds Ratio):
$$
OR = \frac{p_+}{1 - p_+} \in [0, +\infty)
$$</li>
<li>Прологарифмируем шанс, чтобы получить диапазон $(-\infty, +\infty)$:
$$
\log(OR) = \log\left(\frac{p_+}{1 - p_+}\right) = f(x)
$$</li>
</ol>
<p>Выразим вероятность $p_+$ из этого уравнения:
$$
\frac{p_+}{1 - p_+} = e^{f(x)} \implies p_+ = e^{f(x)} - p_+ e^{f(x)} \implies p_+(1 + e^{f(x)}) = e^{f(x)}
$$</p>
<p>Итоговая формула для вероятности (сигмоида):
$$
p_+ = \frac{e^{f(x)}}{1 + e^{f(x)}} = \frac{1}{1 + e^{-f(x)}} = \sigma(f(x))
$$</p>
<p>Таким образом, логистическая регрессия моделирует вероятность принадлежности к классу через сигмоидальную функцию от линейной комбинации признаков.</p>
<h3 id="Понятие-отступа-margin"><a class="header" href="#Понятие-отступа-margin">Понятие отступа (Margin)</a></h3>
<p>Для анализа качества классификации вводится понятие <strong>отступа</strong> (margin) на $i$-м объекте:</p>
<p>$$
M_i = y_i f(x_i) = y_i w^T x_i
$$</p>
<p>где $y_i \in {-1, +1}$ — истинная метка класса.</p>
<p><strong>Интерпретация отступа:</strong></p>
<ul>
<li>$M_i &gt; 0$ — объект классифицирован верно ($y_i = \hat{y}_i$)</li>
<li>$M_i \leq 0$ — объект классифицирован неверно ($y_i \neq \hat{y}_i$)</li>
</ul>
<p><strong>Примеры:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>$y_i$</th><th>$f(x_i)$</th><th>$M_i = y_i f(x_i)$</th><th>Результат</th></tr></thead><tbody>
<tr><td>+1</td><td>+6</td><td>+6</td><td>Верно, уверенно</td></tr>
<tr><td>+1</td><td>-6</td><td>-6</td><td>Ошибка</td></tr>
<tr><td>-1</td><td>-6</td><td>+6</td><td>Верно, уверенно</td></tr>
<tr><td>-1</td><td>+6</td><td>-6</td><td>Ошибка</td></tr>
</tbody></table>
</div>
<p>Чем больше положительный отступ, тем более «уверенно» модель относит объект к правильному классу.
Это понятие лежит в основе многих функций потерь, включая log loss.</p>
<h3 id="Задача-оптимизации-log-loss"><a class="header" href="#Задача-оптимизации-log-loss">Задача оптимизации: Log Loss</a></h3>
<p>Для обучения модели необходимо подобрать веса $w$, максимизирующие правдоподобие данных.
Предположим, что объекты независимы и одинаково распределены (i.i.d.).</p>
<p>Вероятность правильного предсказания для объекта $i$ с меткой $y_i \in {-1, 1}$:
$$
P(y_i | x_i, w) = \sigma(y_i w^T x_i) = \sigma(M_i)
$$</p>
<p>Функция правдоподобия для всей выборки:
$$
P(Y | X, w) = \prod_{i=1}^{N} \sigma(y_i w^T x_i) \to \max
$$</p>
<p>Для удобства оптимизации перейдем к логарифму правдоподобия:
$$
\sum_{i=1}^{N} \log \sigma(y_i w^T x_i) \to \max
$$</p>
<p>Подставив определение сигмоиды $\sigma(z) = \frac{1}{1 + e^{-z}}$, получим:
$$
\sum_{i=1}^{N} \log \frac{1}{1 + e^{-y_i w^T x_i}} = - \sum_{i=1}^{N} \log (1 + e^{-y_i w^T x_i}) \to \max
$$</p>
<p>Задача максимизации логарифма правдоподобия эквивалентна задаче минимизации функции потерь (Log Loss):
$$
\mathcal{L}(w) = \sum_{i=1}^{N} \log (1 + e^{-y_i w^T x_i}) = \sum_{i=1}^{N} \log (1 + e^{-M_i}) \to \min
$$</p>
<p><strong>Связь с отступом:</strong> Функция log loss штрафует малые и отрицательные отступы.
При $M_i \to +\infty$ потери стремятся к нулю, при $M_i \to -\infty$ — растут линейно.</p>
<p><strong>Методы оптимизации:</strong></p>
<ul>
<li>Градиентный спуск</li>
<li>Регуляризация (L1, L2) для борьбы с переобучением, аналогично линейной регрессии</li>
</ul>
<hr />
<h2 id="32-Метрики-качества-классификации"><a class="header" href="#32-Метрики-качества-классификации">3.2. Метрики качества классификации</a></h2>
<p>Выбор правильной метрики критически важен, особенно при несбалансированных классах.</p>
<h3 id="accuracy-и-её-ограничения"><a class="header" href="#accuracy-и-её-ограничения">Accuracy и её ограничения</a></h3>
<p><strong>Accuracy (Доля правильных ответов):</strong>
$$
\text{Accuracy} = \frac{\text{Число верных предсказаний}}{\text{Общее число объектов}}
$$</p>
<p><strong>Проблема:</strong> Accuracy может вводить в заблуждение на несбалансированных данных.</p>
<ul>
<li>Пример: Диагностика редкой болезни.
<ul>
<li>100 000 здоровых, 10 больных.</li>
<li>Если классификатор всегда предсказывает «здоров», Accuracy = $100000 / 100010 \approx 99.99%$.</li>
<li>При этом модель бесполезна, так как не находит ни одного больного.</li>
</ul>
</li>
</ul>
<h3 id="Матрица-ошибок-confusion-matrix"><a class="header" href="#Матрица-ошибок-confusion-matrix">Матрица ошибок (Confusion Matrix)</a></h3>
<p>Для детального анализа используется матрица ошибок:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"></th><th style="text-align: center">Предсказано: 1 (Больной)</th><th style="text-align: center">Предсказано: 0 (Здоровый)</th></tr></thead><tbody>
<tr><td style="text-align: left"><strong>Реально: 1 (Больной)</strong></td><td style="text-align: center"><strong>TP</strong> (True Positive)</td><td style="text-align: center"><strong>FN</strong> (False Negative)</td></tr>
<tr><td style="text-align: left"><strong>Реально: 0 (Здоровый)</strong></td><td style="text-align: center"><strong>FP</strong> (False Positive)</td><td style="text-align: center"><strong>TN</strong> (True Negative)</td></tr>
</tbody></table>
</div>
<ul>
<li><strong>TP:</strong> Сколько больных назвали больными.</li>
<li><strong>TN:</strong> Сколько здоровых назвали здоровыми.</li>
<li><strong>FP:</strong> Сколько здоровых назвали больными (Ошибка I рода).</li>
<li><strong>FN:</strong> Сколько больных назвали здоровыми (Ошибка II рода).</li>
</ul>
<p>$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$</p>
<h3 id="precision-и-recall"><a class="header" href="#precision-и-recall">Precision и Recall</a></h3>
<p>Для задач с дисбалансом классов чаще используют Precision и Recall.</p>
<ol>
<li><strong>Precision (Точность):</strong> Какая доля объектов, названных положительными, действительно является положительными.
$$
\text{Precision} = \frac{TP}{TP + FP}
$$</li>
<li><strong>Recall (Полнота):</strong> Какая доля реальных положительных объектов была найдена моделью.
$$
\text{Recall} = \frac{TP}{TP + FN}
$$</li>
</ol>
<p><strong>Пример 1:</strong> Если модель нашла только 1 больного из 10, но не ошиблась на здоровых:</p>
<ul>
<li>Precision = $1 / (0 + 1) = 1$ (все найденные — действительно больные).</li>
<li>Recall = $1 / (1 + 9) = 0.1$ (найден только 10% больных).</li>
</ul>
<p><strong>Пример 2:</strong> Поменяем целевой класс — будем искать здоровых (100 000 здоровых, 10 больных):</p>
<ul>
<li>Precision = $100000 / (100000 + 9) \approx 0.99991$</li>
<li>Recall = $100000 / (100000 + 0) = 1$</li>
</ul>
<p>Это демонстрирует, что выбор положительного класса влияет на интерпретацию метрик.</p>
<h3 id="f-мера-f-score"><a class="header" href="#f-мера-f-score">F-мера (F-score)</a></h3>
<p>Для баланса между Precision и Recall используется гармоническое среднее — $F_\beta$-мера:</p>
<p>$$
F_\beta = (\beta^2 + 1) \frac{\text{Precision} \times \text{Recall}}{\beta^2 \text{Precision} + \text{Recall}}
$$</p>
<p>Наиболее популярна <strong>F1-мера</strong> ($\beta = 1$):
$$
F_1 = 2 \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$</p>
<h3 id="roc-кривая-и-auc"><a class="header" href="#roc-кривая-и-auc">ROC-кривая и AUC</a></h3>
<p>Многие классификаторы (включая логистическую регрессию) выдают вероятность $p_+ \in [0, 1]$. Порог классификации можно варьировать.</p>
<ul>
<li><strong>TPR (True Positive Rate):</strong> То же самое, что Recall.
$$
\text{TPR} = \frac{TP}{TP + FN}
$$</li>
<li><strong>FPR (False Positive Rate):</strong> Доля здоровых, ошибочно названных больными.
$$
\text{FPR} = \frac{FP}{FP + TN}
$$</li>
</ul>
<p><strong>ROC-кривая (Receiver Operating Characteristic):</strong> Зависимость TPR от FPR при изменении порога классификации.</p>
<p><strong>AUC (Area Under Curve):</strong> Площадь под ROC-кривой.</p>
<ul>
<li>AUC = 1: Идеальный классификатор.</li>
<li>AUC = 0.5: Случайное угадывание.</li>
<li>Чем больше AUC, тем лучше классификатор ранжирует объекты (отделяет положительный класс от отрицательного).</li>
</ul>
<hr />
<h2 id="33-Деревья-решений-decision-trees"><a class="header" href="#33-Деревья-решений-decision-trees">3.3. Деревья решений (Decision Trees)</a></h2>
<p>Дерево решений — это непараметрический метод, который строит последовательность правил для классификации объектов.</p>
<h3 id="Принцип-построения"><a class="header" href="#Принцип-построения">Принцип построения</a></h3>
<p>Представим, что у нас есть один признак. Мы сортируем объекты по нему и выбираем порог $t$, разделяющий выборку на две части: $L$ (left) и $R$ (right).</p>
<p>Формально, для признака $x_i$ и порога $t_j$:
$$
Q \xrightarrow{x_i &lt; t_j} \begin{cases} L \ R \end{cases}
$$</p>
<p>Цель разделения — уменьшить разнородность (гетерогенность) в дочерних узлах. Качество разделения оценивается функцией:
$$
G(x_i, t_j) = \frac{|L|}{|Q|} H(L) + \frac{|R|}{|Q|} H(R)
$$
где $H(R)$ — функция неопределенности (impurity) в узле.</p>
<h3 id="Критерии-неопределенности"><a class="header" href="#Критерии-неопределенности">Критерии неопределенности</a></h3>
<p>Пусть $p_0$ и $p_1$ — доли объектов классов 0 и 1 в узле.</p>
<ol>
<li><strong>Misclassification (Доля ошибок):</strong>
$$
H(R) = 1 - \max(p_0, p_1)
$$</li>
<li><strong>Entropy (Энтропия):</strong>
$$
H(R) = -p_0 \log_2 p_0 - p_1 \log_2 p_1 = - \sum_k p_k \log_2 p_k
$$</li>
<li><strong>Gini (Индекс Джини):</strong>
$$
H(R) = 1 - p_0^2 - p_1^2 = 1 - \sum_k p_k^2
$$</li>
</ol>
<h3 id="Когда-останавливаться-Регуляризация"><a class="header" href="#Когда-останавливаться-Регуляризация">Когда останавливаться? (Регуляризация)</a></h3>
<p>Если не ограничивать рост дерева, оно переобучится (запомнит каждый объект). Критерии остановки:</p>
<ul>
<li>Ограничить максимальную глубину дерева.</li>
<li>Ограничить минимальное количество объектов в узле для дальнейшего деления.</li>
<li>Ограничить минимальное количество объектов в листе.</li>
<li><strong>Pruning (Обрезка):</strong> Построить большое дерево, а затем «постричь» ветви, которые не дают прироста качества на валидации.</li>
</ul>
<h3 id="Плюсы-и-минусы-деревьев-решений"><a class="header" href="#Плюсы-и-минусы-деревьев-решений">Плюсы и минусы деревьев решений</a></h3>
<p><strong>Плюсы:</strong></p>
<ul>
<li><strong>Интерпретируемость:</strong> Правила легко понять и визуализировать.</li>
<li><strong>Масштаб:</strong> Устойчивы к разным масштабам признаков (не требуют нормировки).</li>
<li><strong>Пропуски:</strong> Некоторые реализации могут работать с пропусками в данных.</li>
<li><strong>Параметры:</strong> Мало гиперпараметров для настройки.</li>
</ul>
<p><strong>Минусы:</strong></p>
<ul>
<li><strong>Шум:</strong> Чувствительны к шуму в данных (склонность к переобучению).</li>
<li><strong>Границы:</strong> Разделяющая граница кусочно-линейная (перпендикулярна осям признаков).</li>
<li><strong>Нестабильность:</strong> Малое изменение данных может сильно изменить структуру дерева.</li>
<li><strong>Экстраполяция:</strong> Не умеют экстраполировать (предсказывать за пределами диапазона обучающей выборки), только интерполируют.</li>
</ul>
<hr />
<h2 id="34-Заключение"><a class="header" href="#34-Заключение">3.4. Заключение</a></h2>
<p>В этой лекции мы рассмотрели логистическую регрессию как способ получения вероятностных оценок в линейных моделях и разобрали ключевые метрики для оценки качества классификации, такие как Precision, Recall и ROC-AUC.
Важным понятием оказался <strong>отступ (margin)</strong>, который связывает линейный отклик модели с качеством классификации и лежит в основе функции потерь log loss.</p>
<p>Также мы познакомились с деревьями решений — мощным инструментом, который лежит в основе более сложных алгоритмов.</p>
<p><strong>Что дальше?</strong>
В следующих главах мы рассмотрим:</p>
<ul>
<li>Ансамбли деревьев (Random Forest, Gradient Boosting).</li>
<li>Методы улучшения стабильности и качества деревьев.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="Задача-Реализация-knn-классификатора-с-нуля"><a class="header" href="#Задача-Реализация-knn-классификатора-с-нуля">Задача: Реализация KNN-классификатора "с нуля"</a></h1>
<p>Напишите собственный класс, который реализует алгоритм k ближайших соседей для классификации.</p>
<p>Класс должен поддерживать:</p>
<ol>
<li>Две метрики расстояния: евклидову и манхэттенскую</li>
<li>Два режима голосования: равномерный (<code>uniform</code>) и взвешенный по расстоянию (<code>distance</code>)</li>
<li>Методы <code>fit()</code> и <code>predict()</code></li>
</ol>
<p>Протестируйте реализацию на датасете Iris и Digits, сравните точность с <code>KNeighborsClassifier</code>.
В отчете проанализируйте, как выбор метрики и режима голосования влияет на качество классификации.</p>
<p><a href="https://github.com/phys-dev/knn-task">Ссылка на репозиторий</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="Задание-Реализация-линейной-регрессии-с-нуля"><a class="header" href="#Задание-Реализация-линейной-регрессии-с-нуля">Задание: Реализация линейной регрессии с нуля</a></h1>
<p>Реализуйте три варианта алгоритма линейной регрессии с нуля:</p>
<ul>
<li>Аналитическое решение</li>
<li>Градиентный спуск</li>
<li>Стохастический градиентный спуск</li>
</ul>
<p>Сравнить результаты с реализацией из <code>sklearn</code>, проанализировать влияние гиперпараметров.</p>
<p><a href="https://github.com/phys-dev/linear-regression-task">Ссылка на репозиторий</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
